{
    "contents" : "rm(list = ls())\n\nlibrary(XML)\nlibrary(tm)\n\n# parsing html files. Output: list with words\npath <- 'C:/Users/Manuel/Desktop/Southampton/Data Mining/Text Analysis/gap-html/'\nBooks <- list.files(path)\n# Create empty list for the books\nBookLists <- as.list(NULL)\nfor (i in 1:length(Books)) {\n  Pages <- list.files(paste0(path,Books[i]))\n  for (j in 1:length(Pages)) {\n    if (j == 1) {\n      html.raw<-htmlTreeParse(paste0(path,Books[i], '/', Pages[j]), useInternalNodes=T)\n      page <- xpathApply(html.raw, \"//p/span/span\", xmlValue)\n    } else {\n      html.raw<-htmlTreeParse(paste0(path,Books[i], '/', Pages[j]), useInternalNodes=T)\n      pageN <- xpathApply(html.raw, \"//p/span/span\", xmlValue)\n      page <- c(page, pageN)\n      print(paste0('Book ', i, ':   page ', j, '/', length(Pages)))\n    }\n  }\n  if (i == 1) {\n    BookLists <- page\n  } else {\n    # populate book lists with the lists of words\n    BookLists <- list(BookLists,  page)\n  }\n}\n\nBookList <- list(Book1,Book2,Book3,Book4,Book5,Book6,Book7,Book8,Book9,Book10,Book11,Book12,\n                 Book13,Book14,Book15,Book16,Book17,Book18,Book19,Book20,Book21,Book22,Book23,Book24)\n\n###############################################################################\n#              PREPROCESSING    \nlibrary(tm)\ndocs <- Corpus(VectorSource(BookList))\n\n###### To check how are things going after each step:\ninspect(docs[i])\n\n# remove punctuation\ndocs <- tm_map(docs, removePunctuation)\n\n# rempve numbers\ndocs <- tm_map(docs, removeNumbers)\n# numbers attached to words removed???????\n\n# convert capital letters to lowercase: to group words we need them to be exactly the same\ndocs <- tm_map(docs, tolower)\n\n# remove stopwords:  words with no analytic value (a, the, also.....). \ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\n# For a list of the stopwords, see:   \n# length(stopwords(\"english\"))   \n# stopwords(\"english\")   \n\n\n############################################################################################\n############################################################################################\n############################################################################################\n# Remove specific symbols/strange words: •, •■vtane, €atooaij....\n# However taking out sparse terms later in analysis will remove almost all these terms\n############################################################################################\n############################################################################################\n############################################################################################\n# # remove strange symbols\n# toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, ” “, x))})\n# docs <- tm_map(docs, toSpace, “-“)\n# docs <- tm_map(docs, toSpace, “:”)\n# docs <- tm_map(docs, toSpace, “‘”)\n# docs <- tm_map(docs, toSpace, “•”)\n# docs <- tm_map(docs, toSpace, “■”)\n# docs <- tm_map(docs, toSpace, “—”)\n# docs <- tm_map(docs, toSpace, “►”)\n# docs <- tm_map(docs, toSpace, “€”)\n\n\n\n# remove specific words:\n#docs <- tm_map(docs, removeWords, c(\"word1\", \"word1\" ...))   \n\n# classify different words as the same (acronyms...)\n# for (j in seq(docs))\n# {\n#   docs[[j]] <- gsub(\"qualitative research\", \"QDA\", docs[[j]])\n#   docs[[j]] <- gsub(\"qualitative studies\", \"QDA\", docs[[j]])\n#   docs[[j]] <- gsub(\"qualitative analysis\", \"QDA\", docs[[j]])\n#   docs[[j]] <- gsub(\"research methods\", \"research_methods\", docs[[j]])\n# }\n\n\n# steaming document: (removing common endings: -ing, -es, -s ...)\nlibrary(SnowballC)\ndocs <- tm_map(docs, stemDocument)\n\n# remove white spaces (many as from the processing before we are generating a lot of them)\ndocs <- tm_map(docs, stripWhitespace)\n\n\n###### Treat the preprocessed file as text document\ndocs <- tm_map(docs, PlainTextDocument)\n\n\n\n#                    END OF PREPROCESSING\n\n############################################################################################\n\n# create a document term matrix: each term in column and row the document \ndtm <- DocumentTermMatrix(docs)\n# maybe later we need this matrix transposed\ntdm <- TermDocumentMatrix(docs)\n\n\n# always try if it worked!\n\n############################################################################################\n############################################################################################\n#                EXPLORE DATA\n\n# Organize terms by frequency\nfreq <- colSums(as.matrix(dtm))\nlength(freq)\nord <- order(freq, decreasing = F)\nhead(ord)\ntail(ord)\nhist(freq[ord],1000)\nhist(as.matrix(dtm),100)\n\ndtms <- removeSparseTerms(dtm, 0.1) # This makes a matrix that is 10% empty space, maximum.\ninspect(dtms)\n\ndtms[,1:10]\n\n\n# TODO:\n\n#### ELIMINAR LOS TERMINOS QUE SE REPITEN MUCHO: (50/60 %???? probar y aplicar clustering y algoritmos para comprobar el funcionamiento)\n\n#### td - idf\n#   https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n#### cosine similarity\n\n\n#### hierarquical clustering y otros algoritmos\n#### Nombre de los libros para comprobar resultados!!!\n\n\n\n\nfreq <- colSums(as.matrix(dtms))\nlength(freq)\nord <- order(freq)\nhist(freq[ord],300)\n\nrownames()\n\n# most and less common words\nfreq[tail(ord, 20)]\nfreq[head(ord)]\nas.matrix(dtms[,ord[1730:1745]])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# wf <- data.frame(word=names(freq), freq=freq)\n# head(wf)\n# # plot word frequencies (fancier)\n# library(ggplot2)\n# p <- ggplot(subset(wf, freq>50), aes(word, freq))\n# p <- p + geom_bar(stat=\"identity\")\n# p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))\n# p\n\n\n\n\n\n\n##         READING THE DOCS???\n#################################################################\n## Remove special characters:\n# for(j in seq(docs))   \n# {   \n#   docs[[j]] <- gsub(\"/\", \" \", docs[[j]])   \n#   docs[[j]] <- gsub(\"@\", \" \", docs[[j]])   \n#   docs[[j]] <- gsub(\"\\\\|\", \" \", docs[[j]])   \n# }   \n\n\n### TITULOS DE LOS LIBROS PARA PONERLES TAGS A LOS DOCUMENTOS!: en las primeras paginas en mayuscula los podemos encontrar\n\n",
    "created" : 1457028312641.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4093195632",
    "id" : "7D440F9A",
    "lastKnownWriteTime" : 1458183828,
    "path" : "C:/Users/Manuel/Desktop/Southampton/Data Mining/Text Analysis/code/docParser.R",
    "project_path" : "docParser.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_source"
}